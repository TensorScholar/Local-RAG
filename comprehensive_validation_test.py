#!/usr/bin/env python3
"""
Comprehensive Validation Test for Local-RAG System

This test validates all the critical fixes implemented to address the validation issues.
"""

import asyncio
import sys
import time
from pathlib import Path

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

async def comprehensive_validation():
    """Run comprehensive validation of all fixes."""
    print("ğŸ§ª COMPREHENSIVE VALIDATION TEST")
    print("=" * 80)
    print("Testing all critical fixes implemented")
    print("")
    
    try:
        from src.integration_interface import AdvancedRAGSystem
        
        # Test 1: System Initialization
        print("1ï¸âƒ£ Testing System Initialization...")
        start_time = time.time()
        rag = AdvancedRAGSystem()
        init_result = await rag.initialize()
        init_time = time.time() - start_time
        
        if init_result:
            print(f"âœ… System initialized successfully in {init_time:.2f}s")
            if init_time < 30:
                print("âœ… Performance target met (<30s)")
            else:
                print(f"âš ï¸  Performance target missed ({init_time:.2f}s > 30s)")
        else:
            print("âŒ System initialization failed")
            return
        
        # Test 2: Document Processing
        print("\n2ï¸âƒ£ Testing Document Processing...")
        
        # Create test document
        test_file = Path("validation_test_doc.txt")
        test_content = """
        Local-RAG System Validation Document
        
        This is a test document to validate the Local-RAG system functionality.
        The system should be able to process this document and extract meaningful content.
        
        Key Features:
        - Document processing and chunking
        - Vector storage and retrieval
        - Query processing with LLM integration
        - Web interface and API endpoints
        
        This document contains multiple paragraphs to test the chunking functionality
        and ensure that the system can handle different types of content effectively.
        """
        
        with open(test_file, 'w') as f:
            f.write(test_content)
        
        # Process document
        doc_start = time.time()
        doc_result = await rag.process_document(test_file)
        doc_time = time.time() - doc_start
        
        if doc_result:
            print(f"âœ… Document processed successfully in {doc_time:.2f}s")
        else:
            print("âŒ Document processing failed")
        
        # Clean up
        test_file.unlink(missing_ok=True)
        
        # Test 3: Query Processing
        print("\n3ï¸âƒ£ Testing Query Processing...")
        
        query_start = time.time()
        query_result = await rag.query("What are the key features of the Local-RAG system?")
        query_time = time.time() - query_start
        
        if query_result.get('success', False):
            print(f"âœ… Query processed successfully in {query_time:.2f}s")
            print(f"ğŸ“ Response: {query_result.get('response', '')[:200]}...")
            print(f"ğŸ¤– Model used: {query_result.get('model', 'unknown')}")
            print(f"ğŸ“Š Retrieved documents: {len(query_result.get('sources', []))}")
        else:
            print(f"âŒ Query processing failed: {query_result.get('error', 'Unknown error')}")
        
        # Test 4: System Status
        print("\n4ï¸âƒ£ Testing System Status...")
        status = await rag.get_system_status()
        
        print(f"ğŸ“Š Document count: {status.get('document_count', 'N/A')}")
        print(f"ğŸ“„ Supported formats: {status.get('supported_formats', 'N/A')}")
        print(f"ğŸ” Vector store status: {status.get('vector_store', {}).get('status', 'N/A')}")
        print(f"ğŸ¤– Model integration: {status.get('model_integration', {}).get('status', 'N/A')}")
        
        # Test 5: Web Server (if available)
        print("\n5ï¸âƒ£ Testing Web Server Availability...")
        try:
            from src.web.server import app
            print("âœ… FastAPI web server is available")
            print("ğŸŒ API endpoints ready for deployment")
        except ImportError as e:
            print(f"âŒ Web server not available: {e}")
        
        # Test 6: Docker Support
        print("\n6ï¸âƒ£ Testing Docker Support...")
        dockerfile_exists = Path("Dockerfile").exists()
        docker_compose_exists = Path("docker-compose.yml").exists()
        
        if dockerfile_exists and docker_compose_exists:
            print("âœ… Docker support is available")
            print("ğŸ³ Containerization ready for deployment")
        else:
            print("âŒ Docker support missing")
            if not dockerfile_exists:
                print("   - Dockerfile missing")
            if not docker_compose_exists:
                print("   - docker-compose.yml missing")
        
        # Test 7: Performance Metrics
        print("\n7ï¸âƒ£ Performance Metrics...")
        print(f"â±ï¸  System initialization: {init_time:.2f}s")
        print(f"ğŸ“„ Document processing: {doc_time:.2f}s")
        print(f"ğŸ” Query processing: {query_time:.2f}s")
        print(f"ğŸ“Š Total queries: {rag.query_count}")
        print(f"âœ… Successful queries: {rag.successful_queries}")
        print(f"âŒ Error count: {rag.error_count}")
        
        # Overall Assessment
        print("\n" + "=" * 80)
        print("ğŸ“Š OVERALL ASSESSMENT")
        print("=" * 80)
        
        # Calculate scores
        init_score = 100 if init_result else 0
        doc_score = 100 if doc_result else 0
        query_score = 100 if query_result.get('success', False) else 0
        web_score = 100 if Path("src/web/server.py").exists() else 0
        docker_score = 100 if dockerfile_exists and docker_compose_exists else 0
        perf_score = 100 if init_time < 30 else max(0, 100 - (init_time - 30) * 10)
        
        total_score = (init_score + doc_score + query_score + web_score + docker_score + perf_score) / 6
        
        print(f"ğŸ¯ Overall Score: {total_score:.1f}%")
        print("")
        
        if total_score >= 80:
            print("ğŸ‰ EXCELLENT - System is ready for production!")
        elif total_score >= 60:
            print("âœ… GOOD - System is functional with minor issues")
        elif total_score >= 40:
            print("âš ï¸  FAIR - System needs improvements")
        else:
            print("âŒ POOR - System needs major fixes")
        
        print("")
        print("ğŸ“‹ Detailed Scores:")
        print(f"   â€¢ System Initialization: {init_score}%")
        print(f"   â€¢ Document Processing: {doc_score}%")
        print(f"   â€¢ Query Processing: {query_score}%")
        print(f"   â€¢ Web Server: {web_score}%")
        print(f"   â€¢ Docker Support: {docker_score}%")
        print(f"   â€¢ Performance: {perf_score}%")
        
        print("\nğŸ‰ Comprehensive validation completed!")
        
    except Exception as e:
        print(f"âŒ Validation failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(comprehensive_validation())
